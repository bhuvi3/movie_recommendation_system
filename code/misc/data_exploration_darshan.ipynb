{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_folder = '../../data/CF_User/CFU_val_temp_preds_user/'\n",
    "final_file = '../../data/CF_User/CFU_val_preds_user.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.path.join(orig_folder, fn) for fn in os.listdir(orig_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [pd.read_csv(csv_file) for csv_file in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(final_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639494"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_folder = '../../data/test_stuff/'\n",
    "final_file = '../../data/merged.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.path.join(orig_folder, fn) for fn in os.listdir(orig_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = [pd.read_csv(csv_file) for csv_file in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in parts[1:]:\n",
    "    merged_data = pd.merge(left=merged_data, right=part, on=['movie_id', 'user_id', 'y_true'], validate='one_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625399"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv(final_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF to PredictionHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionHandler(object):\n",
    "    def __init__(self, ground_truth):\n",
    "        self._predictions = {'ground_truth': ground_truth}\n",
    "        self._num_preds = len(ground_truth)\n",
    "        \n",
    "    def add_prediction(self, model_name, predictions):\n",
    "        if len(predictions) != self._num_preds:\n",
    "            raise \"Number of predictions different from the ground truth.\"\n",
    "        self._predictions[model_name] = predictions\n",
    "    \n",
    "    def get_models_list(self):\n",
    "        return list(self._predictions.keys())\n",
    "    \n",
    "    def get_predictions(self, model_name=None):\n",
    "        if model_name and model_name in self._predictions:\n",
    "            return self._predictions[model_name]\n",
    "        else:\n",
    "            return self._predictions\n",
    "\n",
    "\n",
    "def df_to_prediction_handler(df):\n",
    "    y_true = np.array(df['y_true'].values)\n",
    "    predicted_df = df.drop(columns=['user_id', 'movie_id', 'y_true'])\n",
    "    columns = predicted_df.columns\n",
    "    \n",
    "    prediction_handler = PredictionHandler(ground_truth=y_true)\n",
    "    for model_name in columns:\n",
    "        prediction_handler.add_prediction(model_name, \n",
    "                                          np.array(df[model_name].values))\n",
    "        \n",
    "    return prediction_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.22 ms ± 24.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "merged_handler = df_to_prediction_handler(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[625399, 625399, 625399, 625399]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(merged_handler.get_predictions(model)) for model in merged_handler.get_models_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ground_truth', 'CF_User', 'CF_Item', 'LFM']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_handler.get_models_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceAnalyzer(object):\n",
    "    \n",
    "    def __init__(self, prediction_handler: PredictionHandler, roc_thresholds=[3], \n",
    "                 rmse_thresholds=[0], r2_thresholds=[0]):\n",
    "        self._prediction_handler = prediction_handler\n",
    "        self._ground_truth_label = 'ground_truth'\n",
    "        self._model_names = self._prediction_handler.get_models_list()\n",
    "        self._model_names.remove(self._ground_truth_label)\n",
    "        self._roc_thresholds = roc_thresholds\n",
    "        self._rmse_thresholds = rmse_thresholds\n",
    "        self._r2_thresholds = r2_thresholds\n",
    "    \n",
    "    def _euclidean_score(self, y_true, y_pred):\n",
    "        return np.sqrt(np.sum(np.square(y_pred - y_true)))\n",
    "    \n",
    "    \n",
    "    def _roc_auc_score(self, y_true, y_pred):\n",
    "        threshold = 3\n",
    "        if 'threshold' in self._kwargs:\n",
    "            threshold = self._kwargs['threshold']\n",
    "        \n",
    "        # Threshold ground truth ratings\n",
    "        y_true_thresh = np.where(y_true >= threshold, 1, 0)\n",
    "        \n",
    "        # Scale the predictions to bring them to 0-1 range from 0-5 range\n",
    "        y_pred_scaled = y_pred / 5\n",
    "        \n",
    "        # Calculate the area under the ROC curve\n",
    "        area = roc_auc_score(y_true_thresh, y_pred_scaled)\n",
    "        \n",
    "        return area\n",
    "    \n",
    "    \n",
    "    def get_scores(self):\n",
    "        if self._metric_name == 'euclidean':\n",
    "            metric = self._euclidean_score\n",
    "        else:\n",
    "            metric = self._roc_auc_score\n",
    "            \n",
    "        scores = {}\n",
    "        y_true = self._prediction_handler.get_predictions(self._ground_truth_label)\n",
    "        \n",
    "        for model_name in self._model_names:\n",
    "            scores[model_name] = metric(y_true, \n",
    "                                        self._prediction_handler.get_predictions(model_name))\n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    def get_models_list(self):\n",
    "        return self._model_names\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
